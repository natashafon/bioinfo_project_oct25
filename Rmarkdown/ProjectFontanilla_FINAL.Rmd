---
title: "Project October 2025"
author: "Natasha Fontanilla"
date: "2025-11-05"
output:
  pdf_document:
    toc: true        
    toc_depth: 2
    number_sections: false   
    latex_engine: xelatex
fontsize: 10pt
geometry: margin=1in
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
  library(data.table)
```

# Building a DOCKER

The first step is to create a project folder, which will contain all the files needed to build the container. Inside this folder, a file named Dockerfile will be created. The Dockerfile defines the environment and the instructions for building the container.

To create the Dockerfile from the terminal (bash), use:

```{bash echo=FALSE, eval=FALSE}
nano Dockerfile ```
```

Once opened, the Dockerfile can be filled with the required configuration. For this specific project, it was required to install Rstudio inside the docker where all the analysis will be performed.

``` dockerfile
# Start from Ubuntu
FROM ubuntu:22.04

# Avoid prompts during install
ENV DEBIAN_FRONTEND=noninteractive

# Install R and dependencies needed
RUN apt-get update && apt-get install -y \
    r-base \
    gdebi-core \
    wget \
    sudo \
    && apt-get clean

# Install RStudio Server
RUN wget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2024.04.2-764-amd64.deb -O /tmp/rstudio-server.deb && \
    gdebi -n /tmp/rstudio-server.deb && \
    rm /tmp/rstudio-server.deb

# Create RStudio user and password
RUN useradd -m rstudio && echo "rstudio:rstudio" | chpasswd && adduser rstudio sudo

# Install essential R packages
RUN R -q -e "install.packages(c('devtools'), repos='https://cloud.r-project.org')"
RUN R -q -e "install.packages(c('data.table', 'dplyr', 'ggplot2'), repos='https://cloud.r-project.org')"

# Set working directory
WORKDIR /home/project_oct

# Expose RStudio Server port
EXPOSE 8787

# Default command: start RStudio Server
CMD ["/usr/lib/rstudio-server/bin/rserver", "--server-daemonize=0"]
```

To complete the building of the container, open the terminal in the folder containing the Dockerfile and run:

`{bash echo=FALSE, eval=FALSE} docker build -t my_rstudio .`

To run the Docker:

docker run -d -p 8787:8787 -v \$(pwd):/home/project_oct --name project_container my_rstudio

After running, R studio will open and will require a Username and a password defined in the Dockerfile (rstudio). The Rstudio will open.

# Download from GitHub

For this specific project, a custom package called "NF" was created and needs to be downloaded.

In the CONSOLE of R write:

```{r echo=TRUE, eval=FALSE}
install.packages("remotes")
remotes::install_github("natashafon/bioinfo_project_oct25")
```

Load the package:

```{r echo=TRUE, eval=FALSE}
library(NF)
```

```{r echo=TRUE, eval=FALSE}
ls("package:NF")
```

Once the Docker container is running and the repository is installed, the RStudio environment will display a folder named bioinfo_project_oct25. This directory contains all the components required for the project:

-   R — contains all the functions included in the custom NF package.
-   Rscripts — contains the original analysis scripts from which the final functions were derived.
-   R markdown
-   doc -.gitignore
-   Dockerfile
-   LICENSE
-   README.md

Change the directory to begin the analysis.

```{r echo=TRUE}
setwd("~/bioinfo_project_oct25/data")
```

# OVERWIEW

The project is built around data.table package and data.frame R structure. There are 12 tasks and each one will have a data.table and data.frame version.

# TASK 1

The objective of this task is to filter, summarize, and group bulk RNA-seq count data. The analysis uses two input files: bulk_counts_long.csv, which contains gene expression counts per sample, and sample_metadata.csv, which includes sample annotations. The dataset is first filtered to retain only the samples classified as "treated" and the genes whose names begin with "GENE_00". For each gene, the mean and median expression values are then calculated. Next, the filtered count data is joined with the sample metadata to compute per-condition mean expression values in a single, streamlined workflow.

## DATA.TABLE VERSION

```{r, echo=TRUE}
task1_dt <- function(counts_file = "bulk_counts_long.csv",
                     meta_file   = "sample_metadata.csv") {
# Load libraries
  library(data.table)

# Read th CSV files
counts <- fread(counts_file)   # columns: gene, sample_id, count
meta   <- fread(meta_file)    # columns: sample_id, condition, ...

# Ensure sample_id is character
counts[, sample_id := as.character(sample_id)]
meta[,   sample_id := as.character(sample_id)]

# Join on sample_id
merged <- merge(counts, meta, by = "sample_id", sort = FALSE)

# Keep only treated samples and genes starting with "GENE_00"
filtered <- merged[
  condition == "treated" & grepl("^GENE_00", gene)
]


# Mean and median count by gene 
gene_summary <- filtered[, .(
  mean_count   = mean(count, na.rm = TRUE),
  median_count = median(count, na.rm = TRUE)
), by = gene][order(gene)]


# Per-condition mean counts by gene 
per_condition_mean <- merged[, .(
  mean_count = mean(count, na.rm = TRUE)
), by = .(gene, condition)][order(gene, condition)]


return(list(
  merged = merged,
  filtered = filtered,
  gene_summary = gene_summary,
  per_condition_mean = per_condition_mean
))
}

# SHOW RESULTS
res_dt <- task1_dt()
head(res_dt$filtered, 5)
head(res_dt$gene_summary, 5)
head(res_dt$per_condition_mean, 5)

```

## DATA.FRAME VERSION

```{r echo=TRUE}
task1_df <- function(counts_file = "bulk_counts_long.csv",
                     meta_file   = "sample_metadata.csv") {
  
# Read CSV files
counts_df <- read.csv(counts_file, stringsAsFactors = FALSE)
meta_df   <- read.csv(meta_file,  stringsAsFactors = FALSE)

# Ensure sample_id is character
counts_df$sample_id <- as.character(counts_df$sample_id)
meta_df$sample_id   <- as.character(meta_df$sample_id)

# Join on sample_id
merged_df <- merge(counts_df, meta_df, by = "sample_id", sort = FALSE)

# Filter treated + genes starting with "GENE_00"
filtered_df <- merged_df[
  merged_df$condition == "treated" & grepl("^GENE_00", merged_df$gene),
]

# Mean and median by gene (on filtered)
mean_by_gene   <- aggregate(count ~ gene, data = filtered_df, 
                            FUN = function(x) mean(x, na.rm = TRUE))
median_by_gene <- aggregate(count ~ gene, data = filtered_df, 
                            FUN = function(x) median(x, na.rm = TRUE))

# Merge summaries and align column names/order to data.table version

gene_summary_df <- merge(mean_by_gene, median_by_gene, by = "gene")

names(gene_summary_df) <- c("gene", "mean_count", "median_count")

gene_summary_df <- gene_summary_df[order(gene_summary_df$gene), ]

# Per-condition mean counts by gene (on all merged)
per_condition_mean_df <- aggregate(
  count ~ gene + condition,
  data = merged_df,
  FUN = function(x) mean(x, na.rm = TRUE)
)
per_condition_mean_df <- per_condition_mean_df[order(
  per_condition_mean_df$gene, per_condition_mean_df$condition), ]

return(list(
  merged_df = merged_df,
  filtered_df = filtered_df,
  gene_summary_df = gene_summary_df,
  per_condition_mean_df = per_condition_mean_df
))
}

# SHOW RESULTS
res_df <- task1_df()
head(res_df$filtered_df, 5)
head(res_df$gene_summary_df, 5)
head(res_df$per_condition_mean_df, 5)
```

# TASK 2

The goal of this task is to create quality-control style derived variables directly within the count dataset, without producing unnecessary copies of the data in memory. The analysis is based on the file bulk_counts_long.csv, which contains gene expression counts for each sample. First, a log₂-transformed count column is added to the dataset. A binary indicator column (high) is then created, which initially flags samples with counts greater than 100. In the second step, this indicator is refined to be gene-specific: for each gene, high is reassigned based on whether a sample’s count is greater than the gene-wise median count.

## DATA.TABLE VERSION

```{r echo=TRUE}
task2_dt <- function(
    counts_file = "bulk_counts_long.csv") {

# Load the library
library(data.table)

# Read the CSV file
counts_dt <- fread(counts_file)

# Adding log2 counts coulumn which is applied to all rows
counts_dt[, log2_count := log2(count + 1)]

# Adding a binary flag high if count >100 
counts_dt[, high := count > 100]

# Overwrite 'high' if the count is higher thant the median value for a certain gene
counts_dt[, high := count > median(count), by = gene]

return(counts_dt)
}

# SHOW RESULTS
res_dt2 <- task2_dt("bulk_counts_long.csv")
head(res_dt2, 5)
```

### DATA.FRAME VERSION

```{r}
# Read the CSV file
task2_df <- function(path = counts_file) {


#  Read CSV file
counts_df <- read.csv(path, stringsAsFactors = FALSE)

# Add a new column: log2_count
counts_df$log2_count <- log2(counts_df$count + 1)

# Add a column: high = TRUE if count > 100
counts_df$high <- counts_df$count > 100

# Compute median count per gene
med_by_gene <- tapply(counts_df$count, counts_df$gene, median)

# Overwrite 'high' using gene-wise median
counts_df$high <- counts_df$count > med_by_gene[counts_df$gene]

return(counts_df)
}

# SHOW RESULTS
res_df2 <- task2_df("bulk_counts_long.csv")
head(res_df2, 5)


```

# TASK 3

The goal of this task is to increase the efficiency of dataset merging and repeated lookups. The task uses two input files: sample_metadata.csv, which provides sample annotations, and bulk_counts_long.csv, which contains the long-format count measurements. First, the sample_metadata table is indexed by sample_id using setkey(), and an equi-join is performed to merge it directly into the count dataset. Next, a secondary index on (gene, sample_id) is created in the counts table to accelerate filtering and lookup operations. To evaluate the performance improvement, a benchmark is run to compare subset queries before and after indexing.

## DATA.TABLE VERSION

```{r echo=TRUE, message=FALSE, warning=FALSE}
task3_dt <- function(counts_file = "bulk_counts_long.csv",
                     sample_metadata = "sample_metadata.csv") {
  
# Load needed libraries
library(data.table)
library(microbenchmark)

# Read the files
counts_dt3 <- fread(counts_file)
meta_dt3   <- fread(sample_metadata)
  
# Set key and join by sample_id
setkey(meta_dt3, sample_id)
merged_dt3 <- counts_dt3[meta_dt3, on = "sample_id", nomatch = 0L]
  
  
# Benchmark before and after index
gene_query   <- merged_dt3$gene[1]
sample_query <- merged_dt3$sample_id[1]
  
before_time <- microbenchmark(
no_index = merged_dt3[gene == gene_query & sample_id == sample_query],
times = 20L
)

# Add secondary index
setindexv(merged_dt3, c("gene", "sample_id"))
after_time <- microbenchmark(
    with_index = merged_dt3[gene == gene_query & sample_id == sample_query],
    times = 20L
)
  
# Combine benchmark results
benchmark3 <- rbind(
    data.table(expr = "no_index",  median_ms = median(before_time$time) / 1e6),
    data.table(expr = "with_index", median_ms = median(after_time$time) / 1e6)
  )
  benchmark3[, speedup := round(benchmark3$median_ms[1] / benchmark3$median_ms, 2)]
  
return(list(
merged_dt3 = merged_dt3,
benchmark3 = benchmark3
  ))
}

# SHOW RESULTS 
res_dt3 <- task3_dt("bulk_counts_long.csv", "sample_metadata.csv")

head(res_dt3$merged_dt3, 5)

head(res_dt3$benchmark3, 5)

```

## DATA.FRAME VERSION

```{r echo=TRUE, message=TRUE, warning=TRUE}
task3_df <- function(counts_file = "bulk_counts_long.csv",
                     sample_metadata = "sample_metadata.csv") {
  
# Read data
counts_df3 <- read.csv(counts_file, stringsAsFactors = FALSE)
meta_df3   <- read.csv(sample_metadata, stringsAsFactors = FALSE)
  
  
# Merge using base R
merged_df3 <- merge(counts_df3, meta_df3, by = "sample_id", all = FALSE)

# Define query 
gene_query   <- merged_df3$gene[1]
sample_query <- merged_df3$sample_id[1]


# Benchmark
benchmark_df3 <- microbenchmark(
  subset_lookup = merged_df3[merged_df3$gene == gene_query &
                               merged_df3$sample_id == sample_query, ],
  times = 100L
)

# Subset 
subset_df3 <- merged_df3[merged_df3$gene == 
                           gene_query & merged_df3$sample_id == sample_query, ]
  
return(list(
  merged_df3 = merged_df3,
  benchmark_df3 = benchmark_df3,
  subset_df3 = subset_df3))
}


# SHOW RESULTS 

res_df3 <- task3_df("bulk_counts_long.csv", "sample_metadata.csv")

head(res_df3$merged_df3, 5)
head(res_df3$benchmark_df3)
head(res_df3$subset_df3, 5)

```

# TASK 4

The goal of this task is to integrate sample-level metadata with the count matrix and derive summary statistics based on patient information. The analysis uses bulk_counts_long.csv (gene expression counts per sample) together with sample_metadata.csv (which includes patient and condition annotations). The two datasets are first joined, allowing each count value to be associated with the corresponding sample and patient. From this combined dataset, the total expression counts per patient are calculated. Furthermore, within each condition group, the top 10 genes with the highest mean expression values are identified.

## DATA.TABLE VERSION

```{r echo=TRUE}
task4_dt <- function(counts_file = "bulk_counts_long.csv",
                     sample_metadata = "sample_metadata.csv") {
# Load library
library(data.table)
  
# Read both files
counts_dt4 <- fread(counts_file)
meta_dt4   <- fread(sample_metadata)
  
# Annotate counts with metadata (join by sample_id)
counts_annotated <- merge(counts_dt4, meta_dt4, by = "sample_id", all.x = TRUE)
  
# Compute per-patient total counts (then sort by patient_id)
counts_patient_total <- counts_annotated[, .(total_count = sum(count)), 
                                         by = patient_id]
setorder(counts_patient_total, patient_id)   
  
# Find top 10 genes by average count within each condition
counts_top_genes <- counts_annotated[, .(avg_count = mean(count)), 
                                     by = .(condition, gene)]
  
# Order descending and pick top 10 per condition
counts_top10_by_condition <- counts_top_genes[order(condition, -avg_count), 
                                              .SD[1:10], by = condition]

  return(list(
    counts_annotated = counts_annotated,
    counts_patient_total = counts_patient_total,
    counts_top10_by_condition = counts_top10_by_condition
  ))
}

# SHOW RESULTS
res_dt4 <- task4_dt("bulk_counts_long.csv", "sample_metadata.csv")
head(res_dt4$counts_patient_total, 5)
head(res_dt4$counts_top10_by_condition, 5)
```

## DATA.FRAME VERSION

```{r echo=TRUE}
task4_df <- function(counts_file = "bulk_counts_long.csv",
                     sample_metadata = "sample_metadata.csv") {
  
# Read data
counts_df4 <- read.csv(counts_file, stringsAsFactors = FALSE)
meta_df4   <- read.csv(sample_metadata, stringsAsFactors = FALSE)
  
# Annotate counts with metadata (prevent automatic sorting)
counts_annotated <- merge(counts_df4, meta_df4, by = "sample_id", all.x = TRUE, 
                          sort = FALSE)
  
# Compute total counts per patient
counts_patient_totals <- aggregate(count ~ patient_id, data = counts_annotated,
                                   FUN = sum)
colnames(counts_patient_totals)[2] <- "total_count"
  
# Sort the data
counts_patient_totals <- counts_patient_totals[order(
                                      counts_patient_totals$patient_id), ]
  
# Find top 10 genes by average count within each condition
counts_avg_by_condition <- aggregate(count ~ condition + gene, 
                                     data = counts_annotated, FUN = mean)
colnames(counts_avg_by_condition)[3] <- "avg_count"
  
# Order and extract top 10 per condition
counts_top10_genes <- do.call(rbind, lapply(split(counts_avg_by_condition, counts_avg_by_condition$condition), function(x) {
    x <- x[order(-x$avg_count), ]
    head(x, 10)
  }))
  
rownames(counts_top10_genes) <- NULL   # remove control.* rownames
  
  return(list(
    counts_annotated = counts_annotated,
    counts_patient_totals = counts_patient_totals,
    counts_top10_genes = counts_top10_genes
  ))
}

# SHOW RESULTS
res_df4 <- task4_df("bulk_counts_long.csv", "sample_metadata.csv")
head(res_df4$counts_patient_totals, 5)
head(res_df4$counts_top10_genes, 5)
```

# TASK 5

The goal of this task is to classify clinical laboratory measurements according to reference intervals. The analysis uses two main input files: clinical_labs.csv, containing lab measurement values for each patient over time, and lab_reference_ranges.csv, which provides the corresponding lower and upper normal limits for each lab test. The reference ranges are treated as intervals, and each measurement is labeled as either "normal" or "out_of_range" using a non-equi join (i.e., value \>= lower & value \<= upper). After classification, the frequency of abnormal results is summarized both per patient and per lab test, enabling visualization of overall abnormality rates across the cohort.

## DATA.TABLE VERSION

```{r}
task5_dt <- function(labs_file = "clinical_labs.csv",
                     ref_file  = "lab_reference_ranges.csv",
                     meta_file = "sample_metadata.csv") {
  
#Load library
  library(data.table)
  
# Read data
  labs_dt <- fread(labs_file)
  ref_dt  <- fread(ref_file)
  
# Remove duplicate M/F reference ranges
  ref_dt <- unique(ref_dt[, .(lab, lower, upper)])
  
# Merge by lab and classify using vectorised conditions
  merged <- merge(labs_dt, ref_dt, by = "lab", all.x = TRUE)
  
  merged[, status := fifelse(value >= lower & value <= upper,
                             "normal", "out_of_range")]
#Summaries
  abnormal_by_patient_dt5 <- merged[, .(
    total_labs = .N,
    abnormal_n = sum(status == "out_of_range"),
    abnormal_rate = mean(status == "out_of_range")
  ), by = patient_id]
  
  abnormal_by_lab_dt5 <- merged[, .(
    total_patients = .N,
    abnormal_n = sum(status == "out_of_range"),
    abnormal_rate = mean(status == "out_of_range")
  ), by = lab]
  
# Order
setorder(merged, patient_id, time_iso, lab)
setorder(abnormal_by_patient_dt5, -abnormal_rate)
setorder(abnormal_by_lab_dt5, -abnormal_rate)
  
return(list(
    classified_dt5 = merged[, .(patient_id, time_iso, lab, value, status)],
    abnormal_by_patient_dt5 = abnormal_by_patient_dt5,
    abnormal_by_lab_dt5 = abnormal_by_lab_dt5
  ))
}

# SHOW RESULTS
res5 <- task5_dt()
head(res5$classified_dt5)
head(res5$abnormal_by_patient_dt5)
head(res5$abnormal_by_lab_dt5)

```

## DATA.FRAME VERSION

```{r}
task5_df <- function(labs_file = "clinical_labs.csv",
                     ref_file  = "lab_reference_ranges.csv",
                     meta_file = "sample_metadata.csv") {
  
# Read data
  labs_df <- read.csv(labs_file, stringsAsFactors = FALSE)
  ref_df  <- read.csv(ref_file,  stringsAsFactors = FALSE)
  
# Remove duplicate M/F reference ranges
  ref_df <- unique(ref_df[, c("lab", "lower", "upper")])
  
# Merge and classify vectorised
  merged <- merge(labs_df, ref_df, by = "lab", all.x = TRUE)
  merged$status <- ifelse(merged$value >= merged$lower &
                            merged$value <= merged$upper,
                          "normal", "out_of_range")
  
# Summaries 
abnormal_by_patient_df5 <- aggregate(status ~ patient_id, data = merged,
                                       FUN = function(x) mean(x == "out_of_range"))
 names(abnormal_by_patient_df5)[2] <- "abnormal_rate"
  
abnormal_by_patient_df5$total_labs <- as.numeric(table(merged$patient_id))
abnormal_by_patient_df5$abnormal_n <- round(
  abnormal_by_patient_df5$abnormal_rate * abnormal_by_patient_df5$total_labs, 0)
  
abnormal_by_lab_df5 <- aggregate(status ~ lab, data = merged,
                                   FUN = function(x) mean(x == "out_of_range"))
names(abnormal_by_lab_df5)[2] <- "abnormal_rate"
abnormal_by_lab_df5$total_patients <- as.numeric(table(merged$lab))
abnormal_by_lab_df5$abnormal_n <- round(
abnormal_by_lab_df5$abnormal_rate * abnormal_by_lab_df5$total_patients, 0)
  
# Order and return
  merged <- merged[order(merged$patient_id, merged$time_iso, merged$lab), ]
  abnormal_by_patient_df5 <- abnormal_by_patient_df5[
    order(-abnormal_by_patient_df5$abnormal_rate), ]
  abnormal_by_lab_df5 <- abnormal_by_lab_df5[
    order(-abnormal_by_lab_df5$abnormal_rate), ]
  
  return(list(
    classified_df5 = merged[, c("patient_id", "time_iso", "lab", "value", "status")],
    abnormal_by_patient_df5 = abnormal_by_patient_df5,
    abnormal_by_lab_df5 = abnormal_by_lab_df5
  ))
}

# SHOW RESULTS
res5_df <- task5_df()
head(res5_df$classified_df5)
head(res5_df$abnormal_by_patient_df5)
head(res5_df$abnormal_by_lab_df5)
```

# TASK 6

The goal of this task is to classify clinical laboratory measurements according to reference intervals. The analysis uses two main input files: clinical_labs.csv, containing lab measurement values for each patient over time, and lab_reference_ranges.csv, which provides the corresponding lower and upper normal limits for each lab test. (Optionally, sample_metadata.csv can be used to incorporate sex-specific ranges.) The reference ranges are treated as intervals, and each measurement is labeled as either "normal" or "out_of_range" using a non-equi join (i.e., value \>= lower & value \<= upper). After classification, the frequency of abnormal results is summarized both per patient and per lab test, enabling visualization of overall abnormality rates across the cohort.

##  DATA.TABLE VERSION

```{r}
task6_dt <- function(labs_file   = "clinical_labs.csv",
                     vitals_file = "vitals_time_series.csv") {
  
# Load library
library(data.table)
  
# Read files
labs   <- fread(labs_file)
vitals <- fread(vitals_file)
  
# Ensure time columns exist and convert
if (!"time_iso" %in% names(labs))   stop("Column 'time_iso' missing in labs file")
if (!"time_iso" %in% names(vitals)) stop("Column 'time_iso' missing in vitals file")
  
labs[,   lab_time   := as.POSIXct(time_iso)]
vitals[, vital_time := as.POSIXct(time_iso)]
  
# Reshape vitals to wide format (HR + SBP)
vitals_w <- dcast(vitals, patient_id + vital_time ~ vital, value.var = "value")
  
# For each lab, find nearest vital record manually
results_list <- lapply(1:nrow(labs), function(i) {
pid <- labs$patient_id[i]
t0  <- labs$lab_time[i]
subset_v <- vitals_w[vitals_w$patient_id == pid]
    
if (nrow(subset_v) == 0) {
    return(data.table(
        patient_id   = pid,
        lab          = labs$lab[i],
        value        = labs$value[i],
        lab_time     = t0,
        vital_time   = NA,
        HR           = NA_real_,
        SBP          = NA_real_,
        time_lag_min = NA_real_
      ))
    }
    
subset_v[, diff_min := abs(as.numeric(difftime(vital_time, t0, units = "mins")))]
nearest <- subset_v[which.min(diff_min)]
    nearest[, `:=`(
      patient_id   = pid,
      lab          = labs$lab[i],
      value        = labs$value[i],
      lab_time     = t0,
      time_lag_min = diff_min
    )]
    
nearest[, .(patient_id, lab, value, lab_time, vital_time, HR, SBP, time_lag_min)]
  })
  
# Combine all results
  merged_dt6 <- rbindlist(results_list, use.names = TRUE, fill = TRUE)
  
# Correlation summary (CRP only)
crp_dt <- merged_dt6[lab == "CRP"]
corr_summary_dt6 <- crp_dt[, .(
cor_CRP_HR  = cor(value, HR,  use = "complete.obs"),
cor_CRP_SBP = cor(value, SBP, use = "complete.obs")
  ), by = patient_id]
  
  return(list(
    merged_dt6 = merged_dt6,
    corr_summary_dt6 = corr_summary_dt6
  ))
}

# SHOW RESULTS
res_dt6 <- task6_dt()
head(res_dt6$merged_dt6, 5)
head(res_dt6$corr_summary_dt6, 5)

```

## DATA.FRAME VERSION

```{r}
task6_df <- function(labs_file   = "clinical_labs.csv",
                     vitals_file = "vitals_time_series.csv") {
  
# Read CSVs
labs   <- read.csv(labs_file, stringsAsFactors = FALSE)
vitals <- read.csv(vitals_file, stringsAsFactors = FALSE)
  
# Convert time columns
labs$lab_time     <- as.POSIXct(labs$time_iso)
vitals$vital_time <- as.POSIXct(vitals$time_iso)
  
# Reshape vitals to wide (HR + SBP)
vitals_w <- reshape(
vitals[, c("patient_id", "vital_time", "vital", "value")],
    timevar = "vital",
    idvar   = c("patient_id", "vital_time"),
    direction = "wide"
  )
  names(vitals_w) <- gsub("value.", "", names(vitals_w))
  
# For each lab, find nearest vital record
  nearest_row <- function(pid, t0) {
    d <- subset(vitals_w, patient_id == pid)
    if (nrow(d) == 0)
      return(data.frame(time_lab = NA, HR = NA, SBP = NA, time_lag_min = NA))
    diffs <- abs(as.numeric(difftime(d$vital_time, t0, units = "mins")))
    k <- which.min(diffs)
    data.frame(time_lab = d$vital_time[k],
               HR = d$HR[k],
               SBP = d$SBP[k],
               time_lag_min = diffs[k])
  }
  
merged_list <- mapply(nearest_row, labs$patient_id, labs$lab_time, SIMPLIFY = FALSE)
merged_df6  <- cbind(labs[, c("patient_id", "lab", "value", "lab_time")],
                       do.call(rbind, merged_list))
  
# Correlation summary (CRP only)
crp_df <- subset(merged_df6, lab == "CRP")
corr_summary_df6 <- do.call(rbind, lapply(split(crp_df, crp_df$patient_id), function(sub) {
    data.frame(
      patient_id  = unique(sub$patient_id),
      cor_CRP_HR  = cor(sub$value, as.numeric(sub$HR),  use = "complete.obs"),
      cor_CRP_SBP = cor(sub$value, as.numeric(sub$SBP), use = "complete.obs")
    )
  }))
  
  
return(list(
    merged_df6 = merged_df6,
    corr_summary_df6 = corr_summary_df6
  ))
}

# SHOW RESULTS
res_df6 <- task6_df()
head(res_df6$merged_df6, 5)
head(res_df6$corr_summary_df6, 5)
```

# TASK 7

The goal of this task is to filter and rank genomic regions (ATAC-seq peaks) within a specific chromosomal window. The analysis uses atac_peaks.bed.csv, which contains genomic peak coordinates and associated peak scores. The dataset is first restricted to peaks located on chromosome 2, with start coordinates between 2 Mb and 4 Mb. From this filtered subset, peaks are then ranked by score in descending order, and the top 50 highest-scoring peaks are returned as the final output.

## DATA.TABLE VERSION

```{r}
task7_dt <- function(peaks_file = "atac_peaks.bed.csv") {
# Load library
library(data.table)
  
# Read the peaks file
peaks_dt <- fread(peaks_file)

# Subset peaks on chr2 in 2–4 Mb region
chr2_window <- peaks_dt[
  chr == "chr2" & start >= 2e6 & start <= 4e6]
  
# Order by descending score
setorder(chr2_window, -score)
  
# Select top 50 peaks (safe even if <50)
top50_peaks <- head(chr2_window, 50)

  return(list(
    chr2_window = chr2_window,
    top50_peaks = top50_peaks
  ))
}

# SHOW RESULTS
res_dt7 <- task7_dt("atac_peaks.bed.csv")
head(res_dt7$chr2_window, 5)
head(res_dt7$top50_peaks, 5)
```

## DATA.FRAME VERSION

```{r}
task7_df <- function(peaks_file = "atac_peaks.bed.csv") {
# Load library
library(readr)
  
# Read the peaks file
peaks_df <- read_csv(peaks_file)
  
# Show first few lines
head(peaks_df)
  
# Subset peaks on chr2 in 2–4 Mb region
chr2_window_df <- subset(
    peaks_df,
    chr == "chr2" & start >= 2e6 & start <= 4e6
  )
  
# Order by descending score
chr2_window_df <- chr2_window_df[order(-chr2_window_df$score), ]
  
# Select top 50 peaks
top50_peaks_df <- head(chr2_window_df, 50)

return(list(
    chr2_window_df = chr2_window_df,
    top50_peaks_df = top50_peaks_df
  ))
}


# SHOW RESULTS
res_df7 <- task7_df("atac_peaks.bed.csv")
head(res_df7$chr2_window_df, 5)
head(res_df7$top50_peaks_df, 5)

```

# TASK 8

The goal of this task is to perform group-wise summary statistics across multiple columns. The analysis uses bulk_counts_long.csv (gene expression counts) joined with sample_metadata.csv (condition assignments). For each gene within each condition, several robust summary statistics are calculated, including the mean, median, and the first and third quartiles (Q1 and Q3). After computing these summaries, genes are filtered to retain only those for which the mean expression in the treated condition is at least twice the mean expression in the control condition. This identifies genes that demonstrate a strong differential expression pattern between conditions.

## DATA.TABLE VERSION

```{r echo=TRUE, message=FALSE, warning=FALSE}
task8_dt <- function(counts_file = "bulk_counts_long.csv",
                     meta_file   = "sample_metadata.csv") {
  
# Load library 
library(data.table)
  
# Read the CSV files
counts_dt8 <- fread(counts_file)
meta_dt8   <- fread(meta_file)
  
# Merge counts with metadata by sample_id
merged_dt8 <- merge(counts_dt8, meta_dt8, by = "sample_id", sort = FALSE)
  
# Compute per-condition robust summary stats for each gene (mean, median, Q1, Q3)
statsum_dt8 <- merged_dt8[, .(
mean_count   = mean(count, na.rm = TRUE),
median_count = median(count, na.rm = TRUE),
    Q1           = quantile(count, 0.25, na.rm = TRUE),
    Q3           = quantile(count, 0.75, na.rm = TRUE)
  ), by = .(gene, condition)]
  
# View summary
print(head(statsum_dt8))
  
# Reshape to wide format (one row per gene, columns for each condition)
wide_dt8 <- dcast(statsum_dt8, gene ~ condition, value.var = "mean_count")
  
# Filter by keeping only genes where treated mean ≥ 2 × control mean
filtered_dt8 <- wide_dt8[
  !is.na(.SD$treated) &
    !is.na(.SD$control) &
    .SD$treated >= 2 * .SD$control
]

return(list(
  statsum_dt8  = statsum_dt8,
  wide_dt8     = wide_dt8,
  filtered_dt8 = filtered_dt8
  ))
}

# SHOW RESULTS 
counts_dt8_result <- task8_dt()
head(counts_dt8_result$statsum_dt8, 5)
head(counts_dt8_result$wide_dt8, 5)
head(counts_dt8_result$filtered_dt8, 5)
```

## DATA.FRAME VERSION

```{r echo=TRUE, message=FALSE, warning=FALSE}
task8_df <- function(counts_file = "bulk_counts_long.csv",
                     meta_file   = "sample_metadata.csv") {
  
# Load libraries
library(dplyr)
library(tidyr)
library(readr)
  
# Read CSV files
counts_df8 <- read_csv(counts_file)
meta_df8   <- read_csv(meta_file)
  
# Merge counts with metadata by sample_id
merged_df8 <- merge(counts_df8, meta_df8, by = "sample_id", sort = FALSE)
  
# Compute per-condition summary stats for each gene
statsum_df8 <- merged_df8 %>%
group_by(gene, condition) %>%
    summarise(
      mean_count   = mean(count, na.rm = TRUE),
      median_count = median(count, na.rm = TRUE),
      Q1           = quantile(count, 0.25, na.rm = TRUE),
      Q3           = quantile(count, 0.75, na.rm = TRUE),
      .groups = "drop"
    )
  
# View summary
print(head(statsum_df8))
  
# Reshape from long → wide so we can compare control vs treated
wide_df8 <- statsum_df8 %>%
  select(gene, condition, mean_count) %>%
  pivot_wider(names_from = condition, values_from = mean_count)
  
# Filter genes where treated mean ≥ 2 × control mean
filtered_df8 <- wide_df8 %>%
filter(!is.na(treated) & !is.na(control) & treated >= 2 * control)
  
return(list(
statsum_df8  = statsum_df8,
  wide_df8     = wide_df8,
   filtered_df8 = filtered_df8
  ))
}

# SHOW RESULTS 
counts_df8_result <- task8_df()
head(counts_df8_result$statsum_df8, 5)
head(counts_df8_result$wide_df8, 5)
head(counts_df8_result$filtered_df8, 5)

```

# TASK 9

The goal of this task is to perform group-wise summary statistics across multiple columns. The analysis uses bulk_counts_long.csv (gene expression counts) joined with sample_metadata.csv (condition assignments). For each gene within each condition, several robust summary statistics are calculated, including the mean, median, and the first and third quartiles (Q1 and Q3). After computing these summaries, genes are filtered to retain only those for which the mean expression in the treated condition is at least twice the mean expression in the control condition. This identifies genes that demonstrate a strong differential expression pattern between conditions. \## DATA.TABLE VERSION

```{r echo=TRUE, message=FALSE, warning=FALSE}
task9_dt <- function(wide_file = "bulk_counts_wide.csv") {
  
# Load library
library(data.table)
  
# Read CSV file
counts_wide_dt <- fread(wide_file)   # columns: gene, S01, S02, …

str(counts_wide_dt)
  
# Identify numeric columns (sample count columns)
numeric_columns <- setdiff(names(counts_wide_dt), "gene")
numeric_columns  # should list S01, S02, …
  
# Convert from wide to long format
counts_long_dt <- melt(
  counts_wide_dt,
  id.vars = "gene",
  measure.vars = numeric_columns,
  variable.name = "sample_id",
  value.name = "count"
  )
  
setDT(counts_long_dt)

# Compute per-sample totals
counts_long_dt[, sample_total := sum(count, na.rm = TRUE), by = sample_id]

# Assign condition labels (example rule)
counts_long_dt[, condition := ifelse(
  grepl("treat", sample_id, ignore.case = TRUE),
    "treated", "control"
  )]

# Compute mean count per gene × condition
counts_mean_dt <- counts_long_dt[
    , .(mean_count = mean(count, na.rm = TRUE)),
    by = .(gene, condition)
  ][order(gene, condition)]

# Convert back to wide format (gene × condition)
counts_summary_dt <- dcast(
  counts_mean_dt,
  gene ~ condition,
  value.var = "mean_count"
  )

# Set outputs as data.table
setDT(counts_long_dt)
setDT(counts_mean_dt)
setDT(counts_summary_dt)


return(list(
  counts_long_dt    = counts_long_dt,
  counts_mean_dt    = counts_mean_dt,
  counts_summary_dt = counts_summary_dt
  ))
}


# SHOW RESULTS
counts_dt9_result <- task9_dt("bulk_counts_wide.csv")
head(counts_dt9_result$counts_long_dt, 5)
head(counts_dt9_result$counts_mean_dt, 5)
head(counts_dt9_result$counts_summary_dt, 5)
```

## DATA.FRAME VERSION

```{r echo=TRUE, message=FALSE, warning=FALSE}
task9_df <- function(wide_file = "bulk_counts_wide.csv") {
  
# Load library
library(reshape2)
  
# Read CSV file
counts_wide_df <- read.csv(wide_file, stringsAsFactors = FALSE)
head(counts_wide_df)
str(counts_wide_df)
  
# Identify numeric columns (sample count columns)
numeric_columns <- setdiff(names(counts_wide_df), "gene")
numeric_columns  
  
# Convert from wide to long format
counts_long_df <- melt(
counts_wide_df,
id.vars = "gene",
measure.vars = numeric_columns,
variable.name = "sample_id",
value.name = "count"
  )

# Compute per-sample totals
counts_long_df$sample_total <- ave(
counts_long_df$count,
counts_long_df$sample_id,
FUN = function(x) sum(x, na.rm = TRUE)
  )


# Assign condition labels (example rule)
counts_long_df$condition <- ifelse(
  grepl("treat", counts_long_df$sample_id, ignore.case = TRUE),
  "treated", "control"
  )

  
# Compute mean count per gene × condition
counts_mean_df <- aggregate(
  count ~ gene + condition,
  data = counts_long_df,
  FUN = function(x) mean(x, na.rm = TRUE)
)

names(counts_mean_df)[3] <- "mean_count"

counts_mean_df <- counts_mean_df[order(counts_mean_df$gene, 
                                       counts_mean_df$condition), ]

# Convert back to wide format (gene × condition)
counts_summary_df <- reshape(
  counts_mean_df,
  timevar = "condition",
  idvar   = "gene",
  direction = "wide"
)

#Modify names 
names(counts_summary_df) <- gsub("^mean_count\\.", "", 
                                 names(counts_summary_df))

#Sort
col_order <- c("gene", setdiff(sort(names(counts_summary_df)), "gene"))
counts_summary_df <- counts_summary_df[, col_order]


return(list(
  counts_long_df    = counts_long_df,
  counts_mean_df    = counts_mean_df,
  counts_summary_df = counts_summary_df
  ))
}


# SHOW RESULTS
counts_df9_result <- task9_df("bulk_counts_wide.csv")
head(counts_df9_result$counts_long_df, 5)
head(counts_df9_result$counts_mean_df, 5)
head(counts_df9_result$counts_summary_df, 5)
```

# TASK 10

The objective of this task is to map ATAC-seq peaks to gene bodies in order to estimate regulatory signal associated with each gene. The analysis uses two genomic interval datasets: atac_peaks.bed.csv (peak regions with scores) and gene_annotation.bed.csv (gene coordinates). Both datasets are keyed by their genomic coordinates (chr, start, end) to enable efficient interval-based operations. ATAC peaks that overlap gene bodies are identified, and for each gene, two types of summaries are generated: The number of peaks overlapping the gene, and The total number of overlapping base pairs, calculated by determining the overlap length for each peak–gene pair and summing across all peaks for that gene. Finally, the genes are ranked by total overlap length, and the top 20 genes with the highest cumulative peak coverage are returned.

## DATA.TABLE VERSION

```{r}
task10_dt <- function(peaks_file = "atac_peaks.bed.csv",
                      genes_file = "gene_annotation.bed.csv") {
  
library(data.table)
  
#Read data
peaks <- fread(peaks_file)
genes <- fread(genes_file)
  
#Standardize column names
setnames(peaks, 1:3, c("chr", "start", "end"))
setnames(genes, 1:4, c("chr", "gene_start", "gene_end", "gene"))
  
# Set keys
setkey(peaks, chr, start, end)
setkey(genes, chr, gene_start, gene_end)
  
#Intersect peaks with gene bodies
overlaps <- foverlaps(peaks, genes,
                        by.x = c("chr", "start", "end"),
                        type = "any", nomatch = 0L)
  
#Compute overlap length (bp)
overlaps[, overlap_bp := pmin(end, gene_end) - pmax(start, gene_start)]
overlaps <- overlaps[overlap_bp > 0]
  
#Count peaks and sum overlap per gene
peaks_per_gene <- overlaps[, .(
    n_peaks = .N,
    total_overlap_bp = sum(overlap_bp)
  ), by = gene]
  
#Top 20 genes by total overlap
top20 <- peaks_per_gene[order(-total_overlap_bp)][1:20]
  
return(list(
    overlaps_dt10 = overlaps,
    peaks_per_gene_dt10 = peaks_per_gene,
    top20_dt10 = top20
  ))
}

# SHOW RESULTS 
res_dt10 <- task10_dt("atac_peaks.bed.csv", "gene_annotation.bed.csv")
head(res_dt10$overlaps_dt10)
head(res_dt10$peaks_per_gene_dt10)
res_dt10$top20_dt10
```

## DATA.FRAME VERSION

```{r}
task10_df <- function(peaks_file = "atac_peaks.bed.csv",
                      genes_file = "gene_annotation.bed.csv") {
  
# Read data
peaks <- read.csv(peaks_file)
genes <- read.csv(genes_file)

names(peaks)[1:3] <- c("chr", "start", "end")
names(genes)[1:4] <- c("chr", "gene_start", "gene_end", "gene")
  
#Container for overlaps
overlap_list <- list()
  
#Loop per chromosome
  for (ch in intersect(unique(peaks$chr), unique(genes$chr))) {
    p_sub <- subset(peaks, chr == ch)
    g_sub <- subset(genes, chr == ch)
    
#For each gene, find overlapping peaks
for (i in seq_len(nrow(g_sub))) {
      g <- g_sub[i, ]
      hits <- which(p_sub$end > g$gene_start & p_sub$start < g$gene_end)
      
if (length(hits) > 0) {
        tmp <- p_sub[hits, ]
        tmp$gene <- g$gene
        tmp$overlap_bp <- pmin(tmp$end, g$gene_end) - pmax(tmp$start, g$gene_start)
        tmp <- tmp[tmp$overlap_bp > 0, ]
        overlap_list[[length(overlap_list) + 1]] <- tmp
      }
    }
  }
  
# Combine overlaps
overlaps <- do.call(rbind, overlap_list)

# Order 
overlaps <- overlaps[order(overlaps$chr, overlaps$start, overlaps$end), ]

  
#Summarize per gene
peaks_per_gene <- do.call(rbind,
                            lapply(split(overlaps, overlaps$gene), function(x)
                              data.frame(
                                gene = unique(x$gene),
                                n_peaks = nrow(x),
                                total_overlap_bp = sum(x$overlap_bp)
                              ))
  )
  
# Top 20 genes
top20 <- head(peaks_per_gene[order(-peaks_per_gene$total_overlap_bp), ], 20)
  
return(list(
    overlaps_df10 = overlaps,
    peaks_per_gene_df10 = peaks_per_gene,
    top20_df10 = top20
  ))
}


# SHOW RESULTS 
res_df10 <- task10_df("atac_peaks.bed.csv", "gene_annotation.bed.csv")
head(res_df10$overlaps_df10)
head(res_df10$peaks_per_gene_df10)
res_df10$top20_df10
```

# TASK 11

The goal of this task is to map genomic variants to genes by overlapping each variant position with gene coordinates. Variants are converted to 1-bp intervals and matched to genes using interval overlap. The number of HIGH-impact variants is then summarized per gene and per sample, and the list of genes that contain HIGH-impact variants in any sample is reported.

## DATA.TABLE VERSION

```{r}
task11_dt <- function(variants_file = "variants.csv",
                      genes_file    = "gene_annotation.bed.csv") {
  
#Load library 
library(data.table)
  
# Read data
  variants <- fread(variants_file, colClasses = "character")
  genes    <- fread(genes_file,    colClasses = "character")
  
# Convert numeric columns
variants[, pos := as.integer(pos)]
genes[, c("start","end") := .(as.integer(start), as.integer(end))]
  
#Create overlaps 
overlaps_list <- lapply(1:nrow(variants), function(i) {
    v <- variants[i]
    subset <- genes[chr == v$chr & start <= v$pos & end >= v$pos]
    if (nrow(subset) > 0) {
      data.table(chr = v$chr,
                 gene = subset$gene,
                 sample_id = v$sample_id,
                 impact = v$impact)
    } else NULL
  })
  
overlaps <- rbindlist(overlaps_list, use.names = TRUE, fill = TRUE)
  
# Count HIGH-impact variants
counts <- overlaps[impact == "HIGH", .(n_high = .N), by = .(gene, sample_id)]
setorder(counts, gene, sample_id)
  
# Genes with HIGH-impact variants in all samples
n_samples <- uniqueN(variants$sample_id)
genes_all <- counts[, .N, by = gene][N == n_samples, gene]


return(list(
    overlaps_dt11      = overlaps,
    counts_per_gene_dt = counts,
    genes_high_all_dt  = genes_all
  ))
} 
#SHOW RESULTS
res_dt11 <- task11_dt("variants.csv", "gene_annotation.bed.csv")
head(res_dt11$overlaps_dt11, 5)
head(res_dt11$counts_per_gene_dt, 5)
res_dt11$genes_high_all_dt

```

## DATA.FRAME VERSION

```{r}
task11_df <- function(variants_file = "variants.csv",
                      genes_file    = "gene_annotation.bed.csv") {
  
#Read data
variants <- read.csv(variants_file, stringsAsFactors = FALSE)
genes    <- read.csv(genes_file, stringsAsFactors = FALSE)
  
variants$chr <- as.character(variants$chr)
variants$pos <- as.integer(variants$pos)
genes$chr    <- as.character(genes$chr)
genes$start  <- as.integer(genes$start)
genes$end    <- as.integer(genes$end)
genes$gene   <- as.character(genes$gene)
  
# Create overlaps
overlaps <- data.frame(chr=character(), gene=character(),
                         sample_id=character(), impact=character(),
                         stringsAsFactors = FALSE)
  
for (i in 1:nrow(variants)) {
    v <- variants[i, ]
    matches <- subset(genes, chr == v$chr &
                        start <= v$pos &
                        end >= v$pos)
if (nrow(matches) > 0) {
    new_rows <- data.frame(chr = v$chr,
                             gene = matches$gene,
                             sample_id = v$sample_id,
                             impact = v$impact,
                             stringsAsFactors = FALSE)
      overlaps <- rbind(overlaps, new_rows)
    }
  }
  
# Count HIGH-impact variants
high <- subset(overlaps, impact == "HIGH")
if (nrow(high) == 0) {
    counts <- data.frame(gene=character(), sample_id=character(), n_high=integer())
  } else {
    counts <- aggregate(
      list(n_high = rep(1, nrow(high))),
      by = list(gene = high$gene, sample_id = high$sample_id),
      FUN = sum
    )
    counts <- counts[order(counts$gene, counts$sample_id), ]
  }
  
# Genes with HIGH-impact variants in all samples
n_samples <- length(unique(variants$sample_id))
  if (nrow(counts) == 0) {
    genes_all <- character(0)
  } else {
    gene_counts <- aggregate(sample_id ~ gene, data = counts,
                             FUN = function(x) length(unique(x)))
    genes_all <- sort(gene_counts$gene[gene_counts$sample_id == n_samples])
  }
  

  return(list(
    overlaps_df11      = overlaps,
    counts_per_gene_df = counts,
    genes_high_all_df  = genes_all
  ))
}

#SHOW RESULTS
res_df11 <- task11_df("variants.csv", "gene_annotation.bed.csv")
head(res_df11$overlaps_df11, 5)
head(res_df11$counts_per_gene_df, 5)
res_df11$genes_high_all_df
```

# TASK 12

The goal of this task is to combine two study cohorts into a single dataset, ensuring that sample annotations are aligned correctly. The input files cohortA_samples.csv and cohortB_samples.csv are first merged using rbindlist(..., use.names = TRUE, fill = TRUE) to guarantee that columns match even if the two cohorts contain slightly different metadata fields. The combined dataset is then ordered by cohort, condition, and sample_id for consistency. Next, the merged cohort metadata is joined with bulk_counts_long.csv to associate each count measurement with its cohort. From this integrated dataset, the top 100 most variable genes are identified, and mean expression values are computed per cohort and per condition to allow comparison of expression patterns across study groups.

## DATA.TABLE VERSION

```{r}
task12_dt <- function(cohortA_file = "cohortA_samples.csv",
                      cohortB_file = "cohortB_samples.csv",
                      counts_file  = "bulk_counts_long.csv") {
  
# Load library
  library(data.table)

# Read cohort files
A <- fread(cohortA_file)
B <- fread(cohortB_file)
  
#  Combine cohorts
combined <- rbindlist(list(A, B), use.names = TRUE, fill = TRUE)
  
# Order by cohort, condition, sample_id
order_cols <- intersect(c("cohort", "condition", "sample_id"), names(combined))
if (length(order_cols) > 0) setorderv(combined, order_cols)
  
# Read counts and join by sample_id
counts <- fread(counts_file)
merged <- merge(counts, combined, by = "sample_id", allow.cartesian = TRUE)
  
# Compute variance per gene (sorted descending)
var_dt <- merged[, .(variance = var(count, na.rm = TRUE)), by = gene]
var_dt <- var_dt[order(-variance, gene)]
top_genes <- var_dt$gene[1:100]
  
# Compute per-cohort/per-condition mean counts
summary_dt <- merged[gene %in% top_genes,
                       .(mean_count = mean(count, na.rm = TRUE)),
                       by = .(gene, cohort, condition)]
setorderv(summary_dt, c("cohort", "condition", "gene"))
  
# Column alignment summary (silent)
alignment_summary <- data.table(
  Status = c("In both", "Only in A", "Only in B"),
  Count  = c(length(intersect(names(A), names(B))),
               length(setdiff(names(A), names(B))),
               length(setdiff(names(B), names(A))))
  )
  
# Return all results
return(list(
    combined_samples = combined,
    merged_counts    = merged,
    top_genes_var    = var_dt,
    summary_counts   = summary_dt,
    column_check     = alignment_summary
  ))
}

# SHOW RESULTS
res_dt12 <- task12_dt("cohortA_samples.csv", "cohortB_samples.csv", "bulk_counts_long.csv")

head(res_dt12$combined_samples, 5)
head(res_dt12$top_genes_var, 5)
head(res_dt12$summary_counts, 5)
```

## DATA.FRAME VERSION

```{r}
task12_df <- function(cohortA_file = "cohortA_samples.csv",
                      cohortB_file = "cohortB_samples.csv",
                      counts_file  = "bulk_counts_long.csv") {
  
# Read and align cohorts
A <- read.csv(cohortA_file, stringsAsFactors = FALSE)
B <- read.csv(cohortB_file, stringsAsFactors = FALSE)
  
all_cols <- union(names(A), names(B))
for (col in setdiff(all_cols, names(A))) A[[col]] <- NA
for (col in setdiff(all_cols, names(B))) B[[col]] <- NA
  
combined_df <- rbind(A[all_cols], B[all_cols])
  
# Order by cohort, condition, sample_id
  order_cols <- intersect(c("cohort", "condition", "sample_id"), names(combined_df))
  if (length(order_cols) > 0)
    combined_df <- combined_df[do.call(order, combined_df[order_cols]), ]
  
# Read counts and join by sample_id
  counts <- read.csv(counts_file, stringsAsFactors = FALSE)
  merged <- merge(counts, combined_df, by = "sample_id")
  
# Compute variance per gene and select top 100
var_df <- aggregate(count ~ gene, data = merged, FUN = var, na.rm = TRUE)
names(var_df)[2] <- "variance"
var_df <- var_df[order(-var_df$variance, var_df$gene), ]
top_genes <- var_df$gene[1:100]
  
# Compute per-cohort/per-condition mean counts
filtered <- merged[merged$gene %in% top_genes, ]
summary_counts_df <- aggregate(count ~ gene + cohort + condition,
                                 data = filtered,
                                 FUN = function(x) mean(x, na.rm = TRUE))
names(summary_counts_df)[4] <- "mean_count"

summary_counts_df <- summary_counts_df[order(summary_counts_df$cohort,
                                               summary_counts_df$condition,
                                               summary_counts_df$gene), ]
  
# Column alignment summary
  alignment_summary <- data.frame(
    Status = c("In both", "Only in A", "Only in B"),
    Count  = c(length(intersect(names(A), names(B))),
               length(setdiff(names(A), names(B))),
               length(setdiff(names(B), names(A))))
  )
  
return(list(
    combined_samples  = combined_df,
    merged_counts     = merged,
    top_genes_var     = var_df,
    summary_counts_df = summary_counts_df,
    column_check      = alignment_summary
  ))
}

# SHOW RESULTS 
res_df12 <- task12_df("cohortA_samples.csv",
                      "cohortB_samples.csv", "bulk_counts_long.csv")
head(res_df12$combined_samples, 5)
head(res_df12$top_genes_var, 5)
head(res_df12$summary_counts_df, 5)
```

# FINAL REVISION

This task links cell type identities to integration clusters and distinguishes whether cells originate from normal or tumor tissue. The two input tables (integration clusters and cell type annotations) are joined by cell to create a combined reference table. From this, cell type counts are summarized per cluster, and the distribution of cell types is compared between normal and tumor samples. A plot is generated to visualize these distributions, and normalized percentages are computed to compare cluster composition independently of total cell numbers.

## DATA.TABLE VERSION

```{r echo=TRUE}
task13_dt <- function(integration_file = "annotated_GSM3516673_normal_annotated_GSM3516672_tumor_SeuratIntegration.csv",
                      annotation_file  = "nt_combined_clustering.output.csv") {
  
# Load libraries
library(data.table)
library(ggplot2)

# Prepare output directory
if (!dir.exists("output")) dir.create("output")
  
# Read input files
integ_dt <- fread(integration_file)
annot_dt <- fread(annotation_file)
  
# Clean IDs
integ_dt[, cell := gsub("_X_", "", cell)]
  
# Merge
merged_dt13 <- merge(integ_dt, annot_dt, by = "cell", sort = FALSE)
  
# Count per cluster and cell type
count_dt13 <- merged_dt13[, .(count = .N), by = .(integration_cluster, cell_type)]
setorder(count_dt13, integration_cluster, cell_type)
  
# Summary table: add tissue type and normalized percentages
summary_dt13 <- merged_dt13[, .(count = .N),
                              by = .(integration_cluster, cell_type, sample_type)]
summary_dt13[, total := sum(count), by = .(integration_cluster, sample_type)]
summary_dt13[, percent := round((count / total) * 100, 2)]

setorder(summary_dt13, integration_cluster, sample_type, cell_type)

# Order 
summary_dt13[, integration_cluster := factor(integration_cluster,
                                             levels = sort(unique(integration_cluster)))]
summary_dt13[, sample_type := factor(sample_type, levels = c("N", "T"))]
  
# Plot 1: Distribution plot
plot_counts_dt13 <- ggplot(summary_dt13,
                             aes(x = factor(integration_cluster),
                                 y = count,
                                 fill = cell_type)) +
geom_bar(stat = "identity") +
facet_wrap(~sample_type) +
theme_minimal() +
labs(title = "Cell Type Distribution by Cluster and Tissue Type",
         x = "Integration Cluster",
         y = "Number of Cells",
         fill = "Cell Type") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# Plot 2: Distribution plot with normalized percentages
plot_percent_dt13 <- ggplot(summary_dt13,
                              aes(x = factor(integration_cluster),
                                  y = percent,
                                  fill = cell_type)) +
    geom_bar(stat = "identity") +
    facet_wrap(~sample_type) +
    theme_minimal() +
    labs(title = "Cell Distribution by Cluster and Tissue Type (Normalized %)",
         x = "Integration Cluster",
         y = "Percentage (%)",
         fill = "Cell Type") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# Save outputs
fwrite(merged_dt13, "output/merged_dt13.csv")
fwrite(count_dt13, "output/count_dt13.csv")
fwrite(summary_dt13, "output/summary_dt13.csv")
ggsave("output/plot_counts_dt13.pdf", plot_counts_dt13, width = 8, height = 5)
ggsave("output/plot_percent_dt13.pdf", plot_percent_dt13, width = 8, height = 5)
  

  return(list(
    merged_dt13       = merged_dt13,
    count_dt13        = count_dt13,
    summary_dt13      = summary_dt13,
    plot_counts_dt13  = plot_counts_dt13,
    plot_percent_dt13 = plot_percent_dt13
  ))
}

# SHOW RESULTS
res_dt13 <- task13_dt(
  "annotated_GSM3516673_normal_annotated_GSM3516672_tumor_SeuratIntegration.csv",
                      "nt_combined_clustering.output.csv")
head(res_dt13$merged_dt13, 5)
head(res_dt13$count_dt13, 5)
head(res_dt13$summary_dt13, 5)
print(res_dt13$plot_counts_dt13)     
print(res_dt13$plot_percent_dt13)    


```

## DATA.FRAME VERSION

```{r echo=TRUE}
task13_df <- function(integration_file = "annotated_GSM3516673_normal_annotated_GSM3516672_tumor_SeuratIntegration.csv",
                      annotation_file  = "nt_combined_clustering.output.csv") {
  
# Prepare output directory
if (!dir.exists("output")) dir.create("output")
  
# Read input files
integ_df <- read.csv(integration_file, stringsAsFactors = FALSE)
annot_df <- read.csv(annotation_file, stringsAsFactors = FALSE)
  
# Clean IDs
integ_df$cell <- gsub("_X_", "", integ_df$cell)
  
# Merge
merged_df13 <- merge(integ_df, annot_df, by = "cell")
  
# Count per cluster
count_df13 <- as.data.frame(table(
    integration_cluster = merged_df13$integration_cluster,
    cell_type = merged_df13$cell_type
  ))
names(count_df13)[3] <- "count"
count_df13$count <- as.integer(count_df13$count)
count_df13 <- count_df13[count_df13$count > 0, ]
count_df13 <- count_df13[order(count_df13$integration_cluster,
                               count_df13$cell_type), ]
  
# Summary with sample_type and normalized %
summary_df13 <- as.data.frame(table(
    integration_cluster = merged_df13$integration_cluster,
    cell_type = merged_df13$cell_type,
    sample_type = merged_df13$sample_type
  ))
  
names(summary_df13)[4] <- "count"
summary_df13$count <- as.integer(summary_df13$count)
summary_df13 <- summary_df13[summary_df13$count > 0, ]
summary_df13$total <- ave(summary_df13$count,
                            interaction(summary_df13$integration_cluster,
                                        summary_df13$sample_type),
                            FUN = sum)
summary_df13$percent <- round((summary_df13$count / summary_df13$total) * 100, 2)
summary_df13 <- summary_df13[order(summary_df13$integration_cluster,
                                     summary_df13$sample_type,
                                     summary_df13$cell_type), ]
# Order 
summary_df13$integration_cluster <- factor(summary_df13$integration_cluster,
                                           levels = sort(unique(
                                             summary_df13$integration_cluster)))
summary_df13$sample_type <- factor(summary_df13$sample_type,
                                   levels = c("N", "T"))

# Plot 1: Distribution plot
  plot_counts_df13 <- ggplot(summary_df13,
                             aes(x = factor(integration_cluster),
                                 y = count,
                                 fill = cell_type)) +
    geom_bar(stat = "identity") +
    facet_wrap(~sample_type) +
    theme_minimal() +
    labs(title = "Cell Type Distribution by Cluster and Tissue Type",
         x = "Integration Cluster",
         y = "Number of Cells",
         fill = "Cell Type") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# Plot 2: Distribution plot with normalized percentages
  plot_percent_df13 <- ggplot(summary_df13,
                              aes(x = factor(integration_cluster),
                                  y = percent,
                                  fill = cell_type)) +
    geom_bar(stat = "identity") +
    facet_wrap(~sample_type) +
    theme_minimal() +
    labs(title = "Cell Distribution by Cluster and Tissue Type (Normalized %)",
         x = "Integration Cluster",
         y = "Percentage (%)",
         fill = "Cell Type") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# Save outputs
write.csv(merged_df13, "output/merged_df13.csv", row.names = FALSE)
write.csv(count_df13, "output/count_df13.csv", row.names = FALSE)
write.csv(summary_df13, "output/summary_df13.csv", row.names = FALSE)
ggsave("output/plot_counts_df13.pdf", plot_counts_df13, width = 8, height = 5)
ggsave("output/plot_percent_df13.pdf", plot_percent_df13, width = 8, height = 5)
  

return(list(
    merged_df13       = merged_df13,
    count_df13        = count_df13,
    summary_df13      = summary_df13,
    plot_counts_df13  = plot_counts_df13,
    plot_percent_df13 = plot_percent_df13
  ))
}

# SHOW RESULTS 
res_df13 <- task13_df(
  "annotated_GSM3516673_normal_annotated_GSM3516672_tumor_SeuratIntegration.csv",
                      "nt_combined_clustering.output.csv")
head(res_df13$merged_df13, 5)
head(res_df13$count_df13, 5)
head(res_df13$summary_df13, 5)
print(res_df13$plot_counts_df13)     
print(res_df13$plot_percent_df13)    

```
